{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to evals/TinyLlama/TinyLlama-1.1B-Chat-v1.0_1723008419.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import toml\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from src.document_loader import DocumentLoader\n",
    "from src.embeddings import Embeddings\n",
    "from src.vector_store import VectorStore\n",
    "from src.retriever import Retriever\n",
    "from src.llm import LLM\n",
    "from src.rag_agent import RAGAgent\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# env = os.getenv('ENV', 'development')\n",
    "env = 'test'\n",
    "# Load configuration\n",
    "with open(f'config/config.{env}.toml', 'r') as file:\n",
    "    config = toml.load(file)\n",
    "\n",
    "model_id = config['model']['id']\n",
    "document_directory = config['documents']['directory']\n",
    "embedding_directory = os.path.join(document_directory, \"embeddings\")\n",
    "\n",
    "loader = DocumentLoader(document_directory)\n",
    "titles, documents = loader.load_documents()\n",
    "\n",
    "huggingface_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "if not huggingface_api_key:\n",
    "    raise ValueError(\"HUGGINGFACE_API_KEY not found in environment variables\")\n",
    "\n",
    "embeddings = Embeddings(model_id=model_id, HUGGINGFACE_API_KEY=huggingface_api_key)\n",
    "document_embeddings, chunked_texts_with_titles = embeddings.get_embeddings(titles, documents, embedding_directory=embedding_directory)\n",
    "\n",
    "embedding_dimension = len(document_embeddings[0])\n",
    "vector_store = VectorStore(dimension=embedding_dimension)\n",
    "vector_store.add_documents(chunked_texts_with_titles, document_embeddings)\n",
    "\n",
    "retriever = Retriever(vector_store, embeddings)\n",
    "llm = LLM(config['llm']['api_url'])\n",
    "\n",
    "rag_agent = RAGAgent(retriever=retriever, llm=llm)\n",
    "\n",
    "queries = [\n",
    "    \"What is the main theme of the book Dracula?\",\n",
    "    # \"Describe the character of Count Dracula.\",\n",
    "    # \"What are the major settings in the book Dracula?\",\n",
    "    # \"What are the significant symbols used in Dracula?\",\n",
    "    # \"How does the author use foreshadowing in Dracula?\",\n",
    "    # \"What is the role of Mina Harker in Dracula?\",\n",
    "    # \"How does Bram Stoker create suspense in Dracula?\",\n",
    "    # \"What is the significance of blood in Dracula?\",\n",
    "    # \"How does the setting influence the mood in Dracula?\",\n",
    "    # \"What are the Gothic elements in Dracula?\"\n",
    "]\n",
    "\n",
    "average_latencies = []\n",
    "output_speeds = []\n",
    "times = []\n",
    "query_lengths = []\n",
    "results = []\n",
    "\n",
    "for query in queries:\n",
    "    prompt = rag_agent.get_prompt(query=query)\n",
    "    prompt_tokens = len(prompt.split())\n",
    "    \n",
    "    start_time = time.time()\n",
    "    answer = rag_agent.answer(query=query)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response_tokens = len(answer.split())\n",
    "    time_taken = end_time - start_time\n",
    "    average_latency = time_taken\n",
    "    output_speed = response_tokens / time_taken\n",
    "    \n",
    "    average_latencies.append(average_latency)\n",
    "    output_speeds.append(output_speed)\n",
    "    times.append(time_taken)\n",
    "    query_lengths.append(prompt_tokens)\n",
    "    \n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"response_tokens\": response_tokens,\n",
    "        \"time_taken\": time_taken,\n",
    "        \"output_speed\": output_speed,\n",
    "        \"average_latency\": average_latency\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# Summary statistics\n",
    "summary = {\n",
    "    \"average_latency_sec\": sum(average_latencies) / len(average_latencies),\n",
    "    \"output_speed_tokens_sec\": sum(output_speeds) / len(output_speeds),\n",
    "    \"response_time_vs_query_length\": list(zip(times, query_lengths))\n",
    "}\n",
    "\n",
    "# Add summary to results\n",
    "results.append(summary)\n",
    "\n",
    "# Save results to a file\n",
    "timestamp = int(time.time())\n",
    "output_directory = \"evals/\" + model_id.split('/')[0]\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "output_file = os.path.join(output_directory, f\"{model_id.split('/')[-1]}_{timestamp}.json\")\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TinyLlama/TinyLlama-1.1B-Chat-v1.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testmancervenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
